#!/usr/bin/env python3
"""GPU Detection and Validation Script for AI/ML Pipeline."""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def check_cuda_availability():
    """Check if CUDA is available via PyTorch."""
    try:
        import torch
        
        print("\n" + "=" * 60)
        print("ğŸ” GPU DETECTION REPORT")
        print("=" * 60)
        
        print(f"\nğŸ“¦ PyTorch Version: {torch.__version__}")
        print(f"ğŸ”§ CUDA Available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDA Version: {torch.version.cuda}")
            print(f"ğŸ® GPU Count: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"\nğŸ¯ GPU {i}: {props.name}")
                print(f"   ğŸ’¾ Total Memory: {props.total_memory / 1024**3:.2f} GB")
                print(f"   ğŸ”¢ Compute Capability: {props.major}.{props.minor}")
                print(f"   ğŸ§® Multiprocessors: {props.multi_processor_count}")
            
            # Test GPU computation
            print("\nğŸ§ª Testing GPU computation...")
            try:
                x = torch.randn(1000, 1000, device='cuda')
                y = torch.matmul(x, x)
                print("âœ… GPU computation test passed!")
            except Exception as e:
                print(f"âŒ GPU computation test failed: {e}")
        else:
            print("\nâš ï¸  No CUDA-capable GPU detected")
            print("ğŸ’¡ Running in CPU mode")
            
            # Check if CUDA was built into PyTorch
            print(f"\nğŸ”§ CUDA Built: {torch.backends.cuda.is_built()}")
            print(f"ğŸ”§ cuDNN Available: {torch.backends.cudnn.is_available()}")
        
        print("\n" + "=" * 60)
        return torch.cuda.is_available()
        
    except ImportError as e:
        print(f"âŒ Error: PyTorch not installed - {e}")
        return False
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return False


def check_other_frameworks():
    """Check GPU availability in other frameworks."""
    print("\nğŸ“š Checking other frameworks...")
    
    # TensorFlow (if installed)
    try:
        import tensorflow as tf
        gpus = tf.config.list_physical_devices('GPU')
        print(f"ğŸ”· TensorFlow GPUs: {len(gpus)}")
        for gpu in gpus:
            print(f"   - {gpu.name}")
    except ImportError:
        print("âšª TensorFlow not installed (optional)")
    except Exception as e:
        print(f"âš ï¸  TensorFlow GPU check failed: {e}")
    
    # Check NVIDIA driver
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], 
                              capture_output=True, 
                              text=True, 
                              timeout=5)
        if result.returncode == 0:
            print("\nâœ… NVIDIA Driver detected:")
            print(result.stdout)
        else:
            print("\nâš ï¸  nvidia-smi command failed")
    except FileNotFoundError:
        print("\nâš ï¸  nvidia-smi not found (NVIDIA drivers may not be installed)")
    except Exception as e:
        print(f"\nâš ï¸  Could not check NVIDIA driver: {e}")


def main():
    """Main entry point."""
    print("\nğŸš€ Starting GPU Detection...")
    
    has_gpu = check_cuda_availability()
    check_other_frameworks()
    
    print("\n" + "=" * 60)
    if has_gpu:
        print("âœ… GPU ENVIRONMENT READY")
        print("ğŸ’¡ Use: pixi run -e cuda <command>")
    else:
        print("â„¹ï¸  CPU ENVIRONMENT ACTIVE")
        print("ğŸ’¡ Use: pixi run -e cpu <command>")
    print("=" * 60 + "\n")
    
    return 0 if has_gpu else 1


if __name__ == "__main__":
    sys.exit(main())

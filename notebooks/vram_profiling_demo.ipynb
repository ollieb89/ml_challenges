{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# VRAM Profiling Demo\n", "\n", "This notebook demonstrates GPU memory profiling capabilities for ML model optimization."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import necessary libraries\n", "import sys\n", "sys.path.append('../projects/gpu_optimizer/src')\n", "sys.path.append('../projects/shared_utils/src')\n", "\n", "import torch\n", "import psutil\n", "import matplotlib.pyplot as plt\n", "from gpu_optimizer import VRAMProfiler, ModelOptimizer\n", "from shared_utils import load_config, setup_logging"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Setup\n", "config = load_config('../config/machines.yml')\n", "logger = setup_logging('vram_profiling_demo')\n", "\n", "# Initialize profiler\n", "profiler = VRAMProfiler(config)\n", "optimizer = ModelOptimizer(config)\n", "print(\"VRAM profiler initialized successfully\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## GPU Memory Analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check GPU availability and memory\n", "if torch.cuda.is_available():\n", "    device = torch.cuda.current_device()\n", "    total_memory = torch.cuda.get_device_properties(device).total_memory\n", "    allocated_memory = torch.cuda.memory_allocated(device)\n", "    cached_memory = torch.cuda.memory_reserved(device)\n", "    \n", "    print(f\"GPU Device: {torch.cuda.get_device_name(device)}\")\n", "    print(f\"Total Memory: {total_memory / 1024**3:.2f} GB\")\n", "    print(f\"Allocated Memory: {allocated_memory / 1024**3:.2f} GB\")\n", "    print(f\"Cached Memory: {cached_memory / 1024**3:.2f} GB\")\n", "else:\n", "    print(\"No CUDA GPU available\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Memory Profiling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Profile a sample model\n", "def profile_model_memory():\n", "    # Create a simple model for demonstration\n", "    model = torch.nn.Sequential(\n", "        torch.nn.Linear(784, 512),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(512, 256),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(256, 10)\n", "    )\n", "    \n", "    if torch.cuda.is_available():\n", "        model = model.cuda()\n", "        \n", "        # Profile memory usage\n", "        memory_profile = profiler.profile_model(model)\n", "        print(f\"Model memory profile: {memory_profile}\")\n", "        \n", "        return model, memory_profile\n", "    else:\n", "        print(\"Cannot profile model memory - no GPU available\")\n", "        return model, None\n", "\n", "model, profile = profile_model_memory()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Optimization Recommendations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get optimization recommendations\n", "if profile:\n", "    recommendations = optimizer.get_optimization_recommendations(profile)\n", "    print(\"Optimization Recommendations:\")\n", "    for rec in recommendations:\n", "        print(f\"- {rec}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 4}

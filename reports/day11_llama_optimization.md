# Day 11: Llama-7B Memory Optimization Report

**Goal:** Fit Llama-7B (actually Llama-3-8B) into <8GB VRAM.
**Hardware:** NVIDIA GeForce RTX 3070 Ti (8GB VRAM) - Simulated.
**Result:** ✅ Success (5.43 GB Peak VRAM)

## 1. Methodology

We utilized 4-bit Normal Float (NF4) quantization via `bitsandbytes` to reduce the model size.
We targeted the `unsloth/llama-3-8b-bnb-4bit` model, which provides a highly optimized 4-bit quantization of the Llama-3-8B-Instruct model.

### Approaches Tested:

| Method | Estimated VRAM | result on 8GB Card |
| :--- | :--- | :--- |
| **FP16 Baseline** | ~16 GB | ❌ OOM (Theoretical & Tested in Day 6) |
| **INT8 Quantization** | ~9 GB | ❌ OOM (Predicted > 8GB) |
| **INT4 Quantization** | ~5.5 GB | ✅ **SUCCESS** |

## 2. Implementation

The optimization script `scripts/optimize_llama.py` demonstrates:
1. Loading the model with `bitsandbytes` 4-bit config (or using pre-quantized 4-bit weights).
2. Tracing memory usage during inference with `MemoryTracer` (custom tool).
3. Verifying inference capability.

### Code Snippet (`scripts/optimize_llama.py`)
```python
    model = AutoModelForCausalLM.from_pretrained(
        "unsloth/llama-3-8b-bnb-4bit",
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        device_map="auto"
    )
```

## 3. Results (INT4)

Execution Log Summary:
```
INFO:__main__:Initial VRAM: 0.00 GB  
INFO:__main__:Loading unsloth/llama-3-8b-bnb-4bit in INT4...
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0...
INFO:__main__:Model loaded. VRAM: 5.31 GB
INFO:__main__:Running inference...
INFO:__main__:Inference complete in 2.25s
INFO:__main__:Peak VRAM during inference: 5.43 GB
```

### Memory Profile
- **Model Static:** 5.31 GB
- **Activation Peak:** 5.43 GB
- **Headroom:** ~2.2 GB (On 7.6GB Card)

This leaves sufficient room for:
- Context window expansion (KV cache)
- Additional small models (e.g. Pose Estimator) running concurrently (if carefully managed).

## 4. Conclusion

We successfully achieved the Day 11 goal of memory reduction.
- **Reduction:** 16GB (FP16) → 5.43GB (INT4) = **66% Reduction**.
- **Performance:** 2.25s for 20 tokens (~9 tok/s) on cold start. 

The `unsloth` 4-bit model is recommended for all 8GB VRAM deployments.

---
*Generated by Agent Antigravity*
